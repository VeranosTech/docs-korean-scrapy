# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008-2016, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2017.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Scrapy 1.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2017-12-13 13:17+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../docs/faq.rst:4
msgid "Frequently Asked Questions"
msgstr ""

#: ../docs/faq.rst:9
msgid "How does Scrapy compare to BeautifulSoup or lxml?"
msgstr ""

#: ../docs/faq.rst:11
msgid ""
"`BeautifulSoup`_ and `lxml`_ are libraries for parsing HTML and XML. "
"Scrapy is an application framework for writing web spiders that crawl web"
" sites and extract data from them."
msgstr ""

#: ../docs/faq.rst:15
msgid ""
"Scrapy provides a built-in mechanism for extracting data (called "
":ref:`selectors <topics-selectors>`) but you can easily use "
"`BeautifulSoup`_ (or `lxml`_) instead, if you feel more comfortable "
"working with them. After all, they're just parsing libraries which can be"
" imported and used from any Python code."
msgstr ""

#: ../docs/faq.rst:21
msgid ""
"In other words, comparing `BeautifulSoup`_ (or `lxml`_) to Scrapy is like"
" comparing `jinja2`_ to `Django`_."
msgstr ""

#: ../docs/faq.rst:30
msgid "Can I use Scrapy with BeautifulSoup?"
msgstr ""

#: ../docs/faq.rst:32
msgid ""
"Yes, you can. As mentioned :ref:`above <faq-scrapy-bs-cmp>`, "
"`BeautifulSoup`_ can be used for parsing HTML responses in Scrapy "
"callbacks. You just have to feed the response's body into a "
"``BeautifulSoup`` object and extract whatever data you need from it."
msgstr ""

#: ../docs/faq.rst:38
msgid ""
"Here's an example spider using BeautifulSoup API, with ``lxml`` as the "
"HTML parser::"
msgstr ""

#: ../docs/faq.rst:62
msgid ""
"``BeautifulSoup`` supports several HTML/XML parsers. See `BeautifulSoup's"
" official documentation`_ on which ones are available."
msgstr ""

#: ../docs/faq.rst:70
msgid "What Python versions does Scrapy support?"
msgstr ""

#: ../docs/faq.rst:72
msgid ""
"Scrapy is supported under Python 2.7 and Python 3.3+. Python 2.6 support "
"was dropped starting at Scrapy 0.20. Python 3 support was added in Scrapy"
" 1.1."
msgstr ""

#: ../docs/faq.rst:77
msgid ""
"For Python 3 support on Windows, it is recommended to use "
"Anaconda/Miniconda as :ref:`outlined in the installation guide <intro-"
"install-windows>`."
msgstr ""

#: ../docs/faq.rst:81
msgid "Did Scrapy \"steal\" X from Django?"
msgstr ""

#: ../docs/faq.rst:83
msgid ""
"Probably, but we don't like that word. We think Django_ is a great open "
"source project and an example to follow, so we've used it as an "
"inspiration for Scrapy."
msgstr ""

#: ../docs/faq.rst:87
msgid ""
"We believe that, if something is already done well, there's no need to "
"reinvent it. This concept, besides being one of the foundations for open "
"source and free software, not only applies to software but also to "
"documentation, procedures, policies, etc. So, instead of going through "
"each problem ourselves, we choose to copy ideas from those projects that "
"have already solved them properly, and focus on the real problems we need"
" to solve."
msgstr ""

#: ../docs/faq.rst:94
msgid ""
"We'd be proud if Scrapy serves as an inspiration for other projects. Feel"
" free to steal from us!"
msgstr ""

#: ../docs/faq.rst:98
msgid "Does Scrapy work with HTTP proxies?"
msgstr ""

#: ../docs/faq.rst:100
msgid ""
"Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the "
"HTTP Proxy downloader middleware. See "
":class:`~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware`."
msgstr ""

#: ../docs/faq.rst:105
msgid "How can I scrape an item with attributes in different pages?"
msgstr ""

#: ../docs/faq.rst:107
msgid "See :ref:`topics-request-response-ref-request-callback-arguments`."
msgstr ""

#: ../docs/faq.rst:111
msgid "Scrapy crashes with: ImportError: No module named win32api"
msgstr ""

#: ../docs/faq.rst:113
msgid "You need to install `pywin32`_ because of `this Twisted bug`_."
msgstr ""

#: ../docs/faq.rst:119
msgid "How can I simulate a user login in my spider?"
msgstr ""

#: ../docs/faq.rst:121
msgid "See :ref:`topics-request-response-ref-request-userlogin`."
msgstr ""

#: ../docs/faq.rst:126
msgid "Does Scrapy crawl in breadth-first or depth-first order?"
msgstr ""

#: ../docs/faq.rst:128
msgid ""
"By default, Scrapy uses a `LIFO`_ queue for storing pending requests, "
"which basically means that it crawls in `DFO order`_. This order is more "
"convenient in most cases. If you do want to crawl in true `BFO order`_, "
"you can do it by setting the following settings::"
msgstr ""

#: ../docs/faq.rst:138
msgid "My Scrapy crawler has memory leaks. What can I do?"
msgstr ""

#: ../docs/faq.rst:140
msgid "See :ref:`topics-leaks`."
msgstr ""

#: ../docs/faq.rst:142
msgid ""
"Also, Python has a builtin memory leak issue which is described in :ref"
":`topics-leaks-without-leaks`."
msgstr ""

#: ../docs/faq.rst:146
msgid "How can I make Scrapy consume less memory?"
msgstr ""

#: ../docs/faq.rst:148
msgid "See previous question."
msgstr ""

#: ../docs/faq.rst:151
msgid "Can I use Basic HTTP Authentication in my spiders?"
msgstr ""

#: ../docs/faq.rst:153
msgid ""
"Yes, see "
":class:`~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware`."
msgstr ""

#: ../docs/faq.rst:156
msgid "Why does Scrapy download pages in English instead of my native language?"
msgstr ""

#: ../docs/faq.rst:158
msgid ""
"Try changing the default `Accept-Language`_ request header by overriding "
"the :setting:`DEFAULT_REQUEST_HEADERS` setting."
msgstr ""

#: ../docs/faq.rst:164
msgid "Where can I find some example Scrapy projects?"
msgstr ""

#: ../docs/faq.rst:166
msgid "See :ref:`intro-examples`."
msgstr ""

#: ../docs/faq.rst:169
msgid "Can I run a spider without creating a project?"
msgstr ""

#: ../docs/faq.rst:171
msgid ""
"Yes. You can use the :command:`runspider` command. For example, if you "
"have a spider written in a ``my_spider.py`` file you can run it with::"
msgstr ""

#: ../docs/faq.rst:176
msgid "See :command:`runspider` command for more info."
msgstr ""

#: ../docs/faq.rst:179
msgid "I get \"Filtered offsite request\" messages. How can I fix them?"
msgstr ""

#: ../docs/faq.rst:181
msgid ""
"Those messages (logged with ``DEBUG`` level) don't necessarily mean there"
" is a problem, so you may not need to fix them."
msgstr ""

#: ../docs/faq.rst:184
msgid ""
"Those messages are thrown by the Offsite Spider Middleware, which is a "
"spider middleware (enabled by default) whose purpose is to filter out "
"requests to domains outside the ones covered by the spider."
msgstr ""

#: ../docs/faq.rst:188
msgid ""
"For more info see: "
":class:`~scrapy.spidermiddlewares.offsite.OffsiteMiddleware`."
msgstr ""

#: ../docs/faq.rst:192
msgid "What is the recommended way to deploy a Scrapy crawler in production?"
msgstr ""

#: ../docs/faq.rst:194
msgid "See :ref:`topics-deploy`."
msgstr ""

#: ../docs/faq.rst:197
msgid "Can I use JSON for large exports?"
msgstr ""

#: ../docs/faq.rst:199
msgid ""
"It'll depend on how large your output is. See :ref:`this warning <json-"
"with-large-data>` in :class:`~scrapy.exporters.JsonItemExporter` "
"documentation."
msgstr ""

#: ../docs/faq.rst:204
msgid "Can I return (Twisted) deferreds from signal handlers?"
msgstr ""

#: ../docs/faq.rst:206
msgid ""
"Some signals support returning deferreds from their handlers, others "
"don't. See the :ref:`topics-signals-ref` to know which ones."
msgstr ""

#: ../docs/faq.rst:210
msgid "What does the response status code 999 means?"
msgstr ""

#: ../docs/faq.rst:212
msgid ""
"999 is a custom response status code used by Yahoo sites to throttle "
"requests. Try slowing down the crawling speed by using a download delay "
"of ``2`` (or higher) in your spider::"
msgstr ""

#: ../docs/faq.rst:224
msgid ""
"Or by setting a global download delay in your project with the "
":setting:`DOWNLOAD_DELAY` setting."
msgstr ""

#: ../docs/faq.rst:228
msgid "Can I call ``pdb.set_trace()`` from my spiders to debug them?"
msgstr ""

#: ../docs/faq.rst:230
msgid ""
"Yes, but you can also use the Scrapy shell which allows you to quickly "
"analyze (and even modify) the response being processed by your spider, "
"which is, quite often, more useful than plain old ``pdb.set_trace()``."
msgstr ""

#: ../docs/faq.rst:234
msgid "For more info see :ref:`topics-shell-inspect-response`."
msgstr ""

#: ../docs/faq.rst:237
msgid "Simplest way to dump all my scraped items into a JSON/CSV/XML file?"
msgstr ""

#: ../docs/faq.rst:239
msgid "To dump into a JSON file::"
msgstr ""

#: ../docs/faq.rst:243
msgid "To dump into a CSV file::"
msgstr ""

#: ../docs/faq.rst:247
msgid "To dump into a XML file::"
msgstr ""

#: ../docs/faq.rst:251
msgid "For more information see :ref:`topics-feed-exports`"
msgstr ""

#: ../docs/faq.rst:254
msgid "What's this huge cryptic ``__VIEWSTATE`` parameter used in some forms?"
msgstr ""

#: ../docs/faq.rst:256
msgid ""
"The ``__VIEWSTATE`` parameter is used in sites built with ASP.NET/VB.NET."
" For more info on how it works see `this page`_. Also, here's an `example"
" spider`_ which scrapes one of these sites."
msgstr ""

#: ../docs/faq.rst:264
msgid "What's the best way to parse big XML/CSV data feeds?"
msgstr ""

#: ../docs/faq.rst:266
msgid ""
"Parsing big feeds with XPath selectors can be problematic since they need"
" to build the DOM of the entire feed in memory, and this can be quite "
"slow and consume a lot of memory."
msgstr ""

#: ../docs/faq.rst:270
msgid ""
"In order to avoid parsing all the entire feed at once in memory, you can "
"use the functions ``xmliter`` and ``csviter`` from "
"``scrapy.utils.iterators`` module. In fact, this is what the feed spiders"
" (see :ref:`topics-spiders`) use under the cover."
msgstr ""

#: ../docs/faq.rst:276
msgid "Does Scrapy manage cookies automatically?"
msgstr ""

#: ../docs/faq.rst:278
msgid ""
"Yes, Scrapy receives and keeps track of cookies sent by servers, and "
"sends them back on subsequent requests, like any regular web browser "
"does."
msgstr ""

#: ../docs/faq.rst:281
msgid "For more info see :ref:`topics-request-response` and :ref:`cookies-mw`."
msgstr ""

#: ../docs/faq.rst:284
msgid "How can I see the cookies being sent and received from Scrapy?"
msgstr ""

#: ../docs/faq.rst:286
msgid "Enable the :setting:`COOKIES_DEBUG` setting."
msgstr ""

#: ../docs/faq.rst:289
msgid "How can I instruct a spider to stop itself?"
msgstr ""

#: ../docs/faq.rst:291
msgid ""
"Raise the :exc:`~scrapy.exceptions.CloseSpider` exception from a "
"callback. For more info see: :exc:`~scrapy.exceptions.CloseSpider`."
msgstr ""

#: ../docs/faq.rst:295
msgid "How can I prevent my Scrapy bot from getting banned?"
msgstr ""

#: ../docs/faq.rst:297
msgid "See :ref:`bans`."
msgstr ""

#: ../docs/faq.rst:300
msgid "Should I use spider arguments or settings to configure my spider?"
msgstr ""

#: ../docs/faq.rst:302
msgid ""
"Both :ref:`spider arguments <spiderargs>` and :ref:`settings <topics-"
"settings>` can be used to configure your spider. There is no strict rule "
"that mandates to use one or the other, but settings are more suited for "
"parameters that, once set, don't change much, while spider arguments are "
"meant to change more often, even on each spider run and sometimes are "
"required for the spider to run at all (for example, to set the start url "
"of a spider)."
msgstr ""

#: ../docs/faq.rst:309
msgid ""
"To illustrate with an example, assuming you have a spider that needs to "
"log into a site to scrape data, and you only want to scrape data from a "
"certain section of the site (which varies each time). In that case, the "
"credentials to log in would be settings, while the url of the section to "
"scrape would be a spider argument."
msgstr ""

#: ../docs/faq.rst:316
msgid "I'm scraping a XML document and my XPath selector doesn't return any items"
msgstr ""

#: ../docs/faq.rst:318
msgid "You may need to remove namespaces. See :ref:`removing-namespaces`."
msgstr ""

