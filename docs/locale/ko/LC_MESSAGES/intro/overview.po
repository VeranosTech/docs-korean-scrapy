# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008-2016, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2017.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Scrapy 1.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2017-12-13 13:17+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../docs/intro/overview.rst:5
msgid "Scrapy at a glance"
msgstr "스크래피(Scrapy) 한눈에 보기"

#: ../docs/intro/overview.rst:7
msgid ""
"Scrapy is an application framework for crawling web sites and extracting "
"structured data which can be used for a wide range of useful "
"applications, like data mining, information processing or historical "
"archival."
msgstr ""
"스크래피는 웹사이트를 크롤링해서 데이터 마이닝, 정보 프로세싱 또는 기록 아카이브 같은 유용한 어플리케이션에 광법위하게 사용되는 "
"구조화된 데이터를 추출하기 위한 어플리케이션 프레임워크다."

#: ../docs/intro/overview.rst:11
msgid ""
"Even though Scrapy was originally designed for `web scraping`_, it can "
"also be used to extract data using APIs (such as `Amazon Associates Web "
"Services`_) or as a general purpose web crawler."
msgstr ""
"비록 스크래피가 `web scraping`_\\ 을 위해 설계되었지만, (`Amazon Associates Web "
"Services`_ 같은) API를 사용해서 데이터를 추출하거나 범용 웹크롤러로 쓰일 수 있다."

#: ../docs/intro/overview.rst:17
msgid "Walk-through of an example spider"
msgstr "예제 스파이더 살펴보기"

#: ../docs/intro/overview.rst:19
msgid ""
"In order to show you what Scrapy brings to the table, we'll walk you "
"through an example of a Scrapy Spider using the simplest way to run a "
"spider."
msgstr "스크래피가 무엇을 제공하는지 보여주기 위해, 스파이더를 실행하는 가장 쉬운 방법을 사용해서 스크래피 스파이더 예제를 살펴볼 것이다."


#: ../docs/intro/overview.rst:22
msgid ""
"Here's the code for a spider that scrapes famous quotes from website "
"http://quotes.toscrape.com, following the pagination::"
msgstr ""
"아래는 http://quotes.toscrape.com 웹사이트로부터 유명한 인용구를 스크랩하고 페이지 번호를 따라가는 스파이더 "
"코드다::"

#: ../docs/intro/overview.rst:46
msgid ""
"Put this in a text file, name it to something like ``quotes_spider.py`` "
"and run the spider using the :command:`runspider` command::"
msgstr ""
"위 코드를 텍스트 파일에 집어넣고, ``quotes_spider.py``\\ 처럼 이름을 작성하고 "
":command:`runspider` 커맨드를 사용해서 스파이더를 실행시켜라::"

#: ../docs/intro/overview.rst:52
msgid ""
"When this finishes you will have in the ``quotes.json`` file a list of "
"the quotes in JSON format, containing text and author, looking like this "
"(reformatted here for better readability)::"
msgstr ""
"작업이 끝나면 ``quotes.json`` 파일에 텍스트와 저자를 포함하는 JSON 포맷 인용구 리스트가 생길 것이다. 아래처럼 "
"보일 것이다(여기서는 가독성을 위해서 재포맷팅을 했다)::"

#: ../docs/intro/overview.rst:72
msgid "What just happened?"
msgstr "어떤 일이 일어났나?"

#: ../docs/intro/overview.rst:74
msgid ""
"When you ran the command ``scrapy runspider quotes_spider.py``, Scrapy "
"looked for a Spider definition inside it and ran it through its crawler "
"engine."
msgstr ""
"``scrapy runspider quotes_spider.py`` 커맨드를 실행했을 때, 스크래피는 그 안에서 스파이더 정의를 "
"찾고 크롤러 엔진을 통해서 스파이더를 실행시킨다."

#: ../docs/intro/overview.rst:77
msgid ""
"The crawl started by making requests to the URLs defined in the "
"``start_urls`` attribute (in this case, only the URL for quotes in "
"*humor* category) and called the default callback method ``parse``, "
"passing the response object as an argument. In the ``parse`` callback, we"
" loop through the quote elements using a CSS Selector, yield a Python "
"dict with the extracted quote text and author, look for a link to the "
"next page and schedule another request using the same ``parse`` method as"
" callback."
msgstr ""
"크롤링은 ``start_urls`` 속성에서 정의된 (예제의 경우, *humor* 카테고리에 있는 인용구를 위한 URL) URL에 "
"대한 리퀘스트를 생성함으로써 시작되며 기본 콜백 메서드인 ``parse`` 를 호출하고, 리스펀스 객체를 인자로서 전달한다. "
"``parse`` 콜백에서, 우리는 CSS 셀렉터를 사용해 인용구 요소에 대해 루프를 돌리고 추출된 인용구와 텍스트, 저자를 포함한"
" 파이썬 dict를 생산하고, 다음 페이지로 향하는 링크를 찾고 같은 ``parse`` 메서드를 콜백으로 사용해 다른 리퀘스트를 "
"예약한다."

#: ../docs/intro/overview.rst:85
msgid ""
"Here you notice one of the main advantages about Scrapy: requests are "
":ref:`scheduled and processed asynchronously <topics-architecture>`.  "
"This means that Scrapy doesn't need to wait for a request to be finished "
"and processed, it can send another request or do other things in the "
"meantime. This also means that other requests can keep going even if some"
" request fails or an error happens while handling it."
msgstr ""
"여기에서 스크래피의 주요한 이점 중 하나를 알게 되었다: 리퀘스트는 비동기적으로 예약되고 처리된다 :ref:`비동기적으로 예약되고 "
"처리된다 <topics-architecture>`. 이것은 스크래피는 리퀘스트가 종료되고 처리되는 것을 기다릴 필요가 없다. 동시에"
" 다른 리퀘스트를 보내거나 다른 작업을 할 수 있다. 이 말은 다른 리퀘스트는 일부 리퀘스트가 처리될 때 실패하거나 에러가 발생하는"
" 경우에도 계속 진행이 된다는 의미다."

#: ../docs/intro/overview.rst:92
msgid ""
"While this enables you to do very fast crawls (sending multiple "
"concurrent requests at the same time, in a fault-tolerant way) Scrapy "
"also gives you control over the politeness of the crawl through :ref:`a "
"few settings <topics-settings-ref>`. You can do things like setting a "
"download delay between each request, limiting amount of concurrent "
"requests per domain or per IP, and even :ref:`using an auto-throttling "
"extension <topics-autothrottle>` that tries to figure out these "
"automatically."
msgstr ""
"(장애 허용(fault-tolerant) 방식으로 한 번에 다수의 동시적인 리퀘스트를 보내서) 크롤링을 매우 빠르게 해주면서 "
"스크래피는 :ref:`몇 가지 세팅 <topics-settings-ref>`\\ 을 통해서 크롤링의 politeness에 대한 "
"통제권을도 제공한다. 각 리퀘스트 사이의 다운로드 딜레이를 조정하고, 각 도메인이나 IP에 대한 동시적인 리퀘스트의 수를 제한하고,"
" 심지어 자동적으로 이런 것들을 산출하는 :ref:`자동 쓰로틀링(auto-throttling) 확장 <topics-"
"autothrottle>`\\ 을 사용할 수도 있다."

#: ../docs/intro/overview.rst:102
msgid ""
"This is using :ref:`feed exports <topics-feed-exports>` to generate the "
"JSON file, you can easily change the export format (XML or CSV, for "
"example) or the storage backend (FTP or `Amazon S3`_, for example).  You "
"can also write an :ref:`item pipeline <topics-item-pipeline>` to store "
"the items in a database."
msgstr ""
"이 예제는 JSON 파일을 생성하기 위해 :ref:`피드 익스포트(feed exports) <topics-feed-"
"exports>`\\ 를 사용했다. 포맷(XML, CSV 등)이나 스토리지 백엔드(FTP, `Amazon S3`_ 등)는 쉽게 바꿀"
" 수 있다. 또한 :ref:`아이템 파이프라인 <topics-item-pipeline>`\\ 을 만들어서 아이템을 데이터베이스에 "
"저장할 수도 있다."

#: ../docs/intro/overview.rst:111
msgid "What else?"
msgstr "다른 것들은?"

#: ../docs/intro/overview.rst:113
msgid ""
"You've seen how to extract and store items from a website using Scrapy, "
"but this is just the surface. Scrapy provides a lot of powerful features "
"for making scraping easy and efficient, such as:"
msgstr ""
"스크래피를 사용해서 웹사이트로부터 아이템을 추출하고 저장하는 법을 배웠지만, 이것은 시작에 불과하다. 스크래피는 스크랩핑을 쉽고 "
"효율적으로 만드는 강력한 많은 기능들을 제공한다:"

#: ../docs/intro/overview.rst:117
msgid ""
"Built-in support for :ref:`selecting and extracting <topics-selectors>` "
"data from HTML/XML sources using extended CSS selectors and XPath "
"expressions, with helper methods to extract using regular expressions."
msgstr ""
"정규식을 사용한 추출을 가능하게 하는 헬퍼 메서드를 포함해, 확장 CSS Selector와 XPath 표현식을 사용해서 "
"HTML/XML 자료로부터 데이터를 :ref:`선택하고 추출하는 <topics-selectors>` 작업을 위한 빌트인 지원."

#: ../docs/intro/overview.rst:121
msgid ""
"An :ref:`interactive shell console <topics-shell>` (IPython aware) for "
"trying out the CSS and XPath expressions to scrape data, very useful when"
" writing or debugging your spiders."
msgstr ""
"CSS와 XPath 표현식을오 데이터를 스크랩하는 것을 시험해볼 수 있고, 스파이더를 디버깅할 때 매우 유용한 :ref:`대화형 쉘"
" 콘솔 <topics-shell>` (IPython 인식)."

#: ../docs/intro/overview.rst:125
msgid ""
"Built-in support for :ref:`generating feed exports <topics-feed-exports>`"
" in multiple formats (JSON, CSV, XML) and storing them in multiple "
"backends (FTP, S3, local filesystem)"
msgstr ""
"다양한 포맷(JSON, CSV, XML)의 :ref:`피드 익스포트 생성 <topics-feed-exports>`\\ 과 다양한 "
"백엔드로의 저장(FTP, S3, 로컬 파일 시스템)을 위한 빌트인 지원."

#: ../docs/intro/overview.rst:129
msgid ""
"Robust encoding support and auto-detection, for dealing with foreign, "
"non-standard and broken encoding declarations."
msgstr "외국어, 비표준, 망가진 인코딩 선언 처리를 위한 강력한 인코딩 지원과 자동 감지."

#: ../docs/intro/overview.rst:132
msgid ""
":ref:`Strong extensibility support <extending-scrapy>`, allowing you to "
"plug in your own functionality using :ref:`signals <topics-signals>` and "
"a well-defined API (middlewares, :ref:`extensions <topics-extensions>`, "
"and :ref:`pipelines <topics-item-pipeline>`)."
msgstr ""
":ref:`signals <topics-signals>`\\ 을 사용한 사용자 지정 기능성 플러그인을 허용하는 :ref:`강력한 "
"확장성 지원 <extending-scrapy>`, 명확히 정의된 API (미들웨어, :ref:`확장 <topics-"
"extensions>`, :ref:`파이프라인 <topics-item-pipeline>`)."


#: ../docs/intro/overview.rst:137
msgid "Wide range of built-in extensions and middlewares for handling:"
msgstr "광범위한 처리용 미들웨어 및 빌트인 확장:"

#: ../docs/intro/overview.rst:139
msgid "cookies and session handling"
msgstr "쿠키 및 세선 조작"

#: ../docs/intro/overview.rst:140
msgid "HTTP features like compression, authentication, caching"
msgstr "압축, 인증, 캐싱 등의 HTTP 기능"

#: ../docs/intro/overview.rst:141
msgid "user-agent spoofing"
msgstr "사용자-에이전트 스푸핑"

#: ../docs/intro/overview.rst:142
msgid "robots.txt"
msgstr "robots.txt"

#: ../docs/intro/overview.rst:143
msgid "crawl depth restriction"
msgstr "크롤링 깊이 제한"

#: ../docs/intro/overview.rst:144
msgid "and more"
msgstr "기타"

#: ../docs/intro/overview.rst:146
msgid ""
"A :ref:`Telnet console <topics-telnetconsole>` for hooking into a Python "
"console running inside your Scrapy process, to introspect and debug your "
"crawler"
msgstr ""
"크롤러를 검사하고 디버깅하기 위해, 스크래피 프로세스 내에서 실행되고 있는 파이썬 콘솔에 연결하는 :ref:`Telnet "
"console <topics-telnetconsole>`"

#: ../docs/intro/overview.rst:150
msgid ""
"Plus other goodies like reusable spiders to crawl sites from `Sitemaps`_ "
"and XML/CSV feeds, a media pipeline for :ref:`automatically downloading "
"images <topics-media-pipeline>` (or any other media) associated with the "
"scraped items, a caching DNS resolver, and much more!"
msgstr ""
"`Sitemaps`_\\ 과 XML/CSV 피드 있는 사이트를 크롤링하는 재사용 가능한 스파이더와, 스크랩된 아이템과 연결된 "
":ref:`자동 이미지(또는 다른 미디어) 다운로드 <topics-media-pipeline>`\\ 용 미디어 파이프라인, 캐싱 "
"DNS resolver, 등"

#: ../docs/intro/overview.rst:156
msgid "What's next?"
msgstr "다음 단계는?"

#: ../docs/intro/overview.rst:158
msgid ""
"The next steps for you are to :ref:`install Scrapy <intro-install>`, "
":ref:`follow through the tutorial <intro-tutorial>` to learn how to "
"create a full-blown Scrapy project and `join the community`_. Thanks for "
"your interest!"
msgstr ""
"다음 단계는 :ref:`스크래피를 설치하는 <intro-install>` 것이며, :ref:`튜토리얼을 따라서 <intro-"
"tutorial>` 완전한 스크래피 프로젝트를 만드는 방법을 배우고 `커뮤니티에 참여하기`_ 바란다."
